{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd3b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9708b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35138a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2868d898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c976796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1afb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386332a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efce0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b42df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6adbef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e1c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab0a4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b596bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c11f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bc71379",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be92da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c0b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e91d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee6259f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c3c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02ed70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c37670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21824583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce10811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e44bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9239c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b77e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4662629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "387886be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  5, 29, 34,  6,  0, 20,  9, 61, 63,  5, 30, 12, 33, 27, 27,  5,\n",
       "       56, 65,  2, 25, 53, 24, 54, 33, 10,  2, 50,  4, 41, 60,  2, 40, 20,\n",
       "       43, 42, 57, 28, 41,  2, 64, 38, 15,  5, 28, 42, 39, 26, 39, 42, 45,\n",
       "       58, 50, 22, 39, 37, 34,  9, 39, 52,  9,  7, 33, 53,  3, 29, 26, 35,\n",
       "       18, 40, 40, 27, 12, 46,  7, 25, 44,  7, 60, 27, 10, 19, 55, 49, 45,\n",
       "       11, 24,  8, 14,  9, 34, 32, 52, 59,  9, 58, 30, 25,  9, 12],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4561611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"se up from the ground\\nTill Bolingbroke have pardon'd thee. Away, be gone!\\n\\nHENRY BOLINGBROKE:\\nCan no\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"3&PU'[UNK]G.vx&Q;TNN&qz LnKoT3 k$bu aGdcrOb yYB&OcZMZcfskIZXU.Zm.,Tn!PMVEaaN;g,Le,uN3Fpjf:K-A.USmt.sQL.;\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "612f3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1720c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190234, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31156a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.03825"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da70b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "229d3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40f4ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 223s 1s/step - loss: 2.7028\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 222s 1s/step - loss: 1.9800\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 228s 1s/step - loss: 1.7012\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 231s 1s/step - loss: 1.5408\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 227s 1s/step - loss: 1.4433\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 221s 1s/step - loss: 1.3761\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 226s 1s/step - loss: 1.3235\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 213s 1s/step - loss: 1.2786\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 216s 1s/step - loss: 1.2380\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 224s 1s/step - loss: 1.1981\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 227s 1s/step - loss: 1.1579\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 222s 1s/step - loss: 1.1171\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 214s 1s/step - loss: 1.0735\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 225s 1s/step - loss: 1.0274\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 224s 1s/step - loss: 0.9790\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 226s 1s/step - loss: 0.9278\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 233s 1s/step - loss: 0.8774\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 230s 1s/step - loss: 0.8252\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 223s 1s/step - loss: 0.7742\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 231s 1s/step - loss: 0.7273\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ead594eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89ecf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c85b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I warrant it is: this had been captive as idness,\n",
      "Nexty! how to bee so great, and I will not you sleep.\n",
      "\n",
      "Camonious danger out.\n",
      "\n",
      "VIRGILIA:\n",
      "No longer, make was never graced before him:\n",
      "Though tortain, mark no man might have ear\n",
      "Than a properly superfull il those free\n",
      "For this depart; I could forget with thee.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "The sun is here!\n",
      "Put on this cause with me to be a jud\n",
      "an shaper kernels of what thou hast broke a bawd.\n",
      "\n",
      "LEONTES:\n",
      "No; if I did hear from the coroous of your voices. Say, so soldiers,\n",
      "Will thereby hear these rass that did go two;\n",
      "O gracious face, levie, that you read.\n",
      "A Dozal durth, my lord, with all these mine!\n",
      "Masters so much upon my guess,\n",
      "That hath For executioner, and rob her heaps,\n",
      "Thy mother's daughter shall write on our late encompany this\n",
      "fair grave shall I re past like a great a woman of the woman?\n",
      "The apt that, which will share with kings and honours and at home\n",
      "That answer forth a trutty,\n",
      "And nothing to bed, was into a holy peorge\n",
      "And vows to part them.' \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.3044376373291016\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "512b68e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThou wrong'st the brother of a down, consemn;\\nFor, by their aiming bitter cowardice\\nAnd mayor knock me how is he before themselves;\\nFor then our life the ran make haste,\\nMine eyes to death, is nothing; nor the Volsces\\nAre what they are: die, son, Walliking wits thyself?\\n\\nGLOUCESTER:\\nKnath mayst thou, speak sad my lord of Buckingham:\\nTravelling of my side, to pardon him,\\nThat rather will no other prodigy\\nBut with our cousin cousin, which is no screet,\\nHortensio, 'tis no more honour, like a dismal tempeed strivet;\\nParce love to her, infolence, I wot well with him:\\n\\nBUSHY:\\nDoth make me go men was a mile and glory\\nWith physficationants, off forser against my woe.\\n\\nRIVERS:\\nTake my leave: and you shall bear my boy\\nFrom manner down, and that thy sword upon thy face\\nWhich, that may be, sir, at nothing to marry it,\\nMe dangerously speak against my head\\nAs often ere morning:\\nNay, where is my son was being while the roof: and, since\\nI cannot think there's many comes with man:\\nEither was thy name \"\n",
      " b\"ROMEO:\\nThou shouldst less short.\\n\\nPERDITA:\\nHow is't, sweet bed, Mucat\\nAs thou wert not; but your will not stay, he look\\nupon their caps and scaldity, and bravely governow\\nWhich had the people in the turre to\\nputs it with my land\\nAt make a goad with a gentle king,\\nAnd burn betwaining these blooder a tarth.\\nCall your own beauty would he be meet you.\\n\\nDUKE VINCENTIO:\\nWhat doth our speed? do they were heavens?\\n\\nVOLUMNIA:\\nO, 'twere pleased my fault, to prescript,\\nTo your own senses and the garments but\\nwhat I am, rancour castimes, and hold your made\\nIf he were so, as as first to relen him up\\nAt rashly stain her tut; and I,--its are\\nWith sweethern England's king, and many flowers all at modesty,\\nAnd, if thy best I sent her report:\\nI'll not to God, my Lord Northumberland, I the day:\\nShe will be so, believe not God, that which is Clifford fled:\\nBy every scoul I cannot, be said by then ensue.\\n\\nJULIET:\\nAt these enjointed, whome, whom, those children's chamber up a propeg thee,\\nAnd seen it in the trial\"\n",
      " b\"ROMEO:\\nThe window of I knife with this be late I had.\\nI have no chiders and my state to the people, by Somerseter,\\nThan thou hast made me stay awhowing\\nI knew that such as we\\ncall them to our about in so relied,\\nAm I but this of France behooding of the king,\\nAnd with my rapen'r heads and pray that woe\\nfor mets me well: and le, the heavens at an--dught, and\\nlive and weaken o'erphished without that remembrance\\nWrets from heavenly course the time o'erween points;\\nAnd, by the brother blind in fame, and mine,\\nDo as the moon came in murders must my happiness;\\nFor What is please my lady's cushions but\\nIself pound of rude in Galiently.\\n\\nShepherd:\\nThat's off wreaty; for he hath intends\\nme as so disselfies that I should remorted\\nIn mortal glory to hither so\\nThat have good lady to our friends,\\nYour susting and i' the bodies of his soul!\\nThe wanton begins to speak, slain, too richest lies,\\nThou wilt not be abused in't it;\\nBut I say, such a yield unrespalled these suppsible.\\n\\nEMerd:\\nAy, as they breathed \"\n",
      " b\"ROMEO:\\nThat hadst thou sin'st ensortency and outs we grant.\\n\\nKING HENRY VI:\\nO Plabent anon, Daunting Venou comes\\nBe something in my face, I say he must.\\n\\nSecond Citizen:\\nAy, but this is not so.\\n\\nKING EDWARD IV:\\n\\nCLAUDIO:\\nMarry, sir, is it come.\\n\\nSICINIUS:\\nMark you, great sorrow; let me go along with her\\nto put not wit; and why he is within, sir, hath done\\nTake on the way our fears; but with not offer,\\nHe'll be the right of this smoot-born be.\\nFor my good nurse, that I should live the heavy rich!\\nHear you thyself, both dismal him,--\\nSir, that you would not have some consent\\nI' the year; there five and leaves unto my flade.\\n\\nESCALUS:\\nA rotter curcise, Or I'll be watch'd good.\\nIE patience.\\n\\nWARWICK:\\nHe would not count distolm our throats, yea, and myself\\nFrongin our hearts with this regroon-warm distenter,\\nThou hast no cause; hear upon their table,\\nNette backws, henceforth murder, that is yonded again.\\nWe--while hearing myself hath long before me.\\n\\nPROSPERO:\\nWho willingly comfort!\\nBy assing and\"\n",
      " b\"ROMEO:\\n\\nJULIET:\\nThat vows that hath he thrive more toward!\\nCORILLO:\\nGood my lord's a life,\\nShe humble health, in follow me and do wrong for your days\\nAnd bandy than my prophecy, and now quite lost by\\nA shape--\\nStayvel to your husband was your Romeo's honour is to be sour.\\n\\nShepherd:\\nThink you, sir, soft! which of you this advantle to me.\\nInjury, that hast more curreit us.\\n\\nSecond Citizen:\\nAy, so I did will make our friends,\\nNot so, ho, if it please my looks. Come, I would have show'd him:\\nFor it was to before 'em.\\n\\nClown:\\nCome on; Who's there?\\n\\nBENVOLIO:\\nFarewell: my brother, Ereling: howsoe!\\nThou art not joy?\\nO cursed be plainless, will they not\\nAmiss us over-of here in his absence; and him I am an iron,\\nHave struck no frights so hurt and all respects,\\n'Twere ne'er a creature to prosper, uncle, in post to thee:\\nIf you can think the queen which blood alike\\nShores no man dropp'd, against this world end. This is that gain of course.\\n\\nJOHN OF GAUNT:\\nO, tired lady, she's dead; alas! this is not \"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.665024995803833\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96ed6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000021E0100A2E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7610eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "She wouldst, and devilable strength and lips.\n",
      "\n",
      "JULIET:\n",
      "I saw how swear it, stift his bed; and I\n",
      "wil\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb7528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
